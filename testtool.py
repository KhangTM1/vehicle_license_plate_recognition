# -*- coding: utf-8 -*-
"""testTool.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GZkQl0r72LJe0DZzNxNapQX3jfzv8jVz
"""

from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import cv2
from google.colab.patches import cv2_imshow

"""Sau khi tính độ sáng ảnh sẽ đưa qua một phương trình phi tuyến để đưa ra một chỉ số threshold thích hợp cho việc nhị phân ảnh

"""

def gaussian_filter(image, kernel_size, sigma):
    kernel = np.fromfunction(
        lambda x, y: (1/ (2*np.pi*sigma**2)) * np.exp(- ((x-(kernel_size-1)/2)**2 + (y-(kernel_size-1)/2)**2) / (2*sigma**2)),
        (kernel_size, kernel_size)
        # G(x,y)=e^(-((x**2)+(y**2))/2σ**2)/(2πσ**2)
    )
    kernel = kernel / np.sum(kernel)

    output_image = np.zeros_like(image, dtype=float)
    pad_width = kernel_size // 2

    for i in range(pad_width, image.shape[0]-pad_width):
        for j in range(pad_width, image.shape[1]-pad_width):
            region = image[i-pad_width:i+pad_width+1, j-pad_width:j+pad_width+1, :]
            output_image[i, j, 0] = np.sum(region[:, :, 0] * kernel)
            output_image[i, j, 1] = np.sum(region[:, :, 1] * kernel)
            output_image[i, j, 2] = np.sum(region[:, :, 2] * kernel)

    return np.uint8(output_image)

"""hàm là bộ lọc gauss nhằm mục tiêu làm mờ để lọc nhiễu ảnh
đây là giải thích về cách thuật toán hoạt động:

Tạo kernel Gaussian: Kernel là một ma trận có kích thước (kernel_size, kernel_size) được tạo dựa trên công thức hàm Gaussian. Kernel này chứa các trọng số, trong đó giá trị ở trung tâm cao hơn và giảm dần khi đi xa khỏi trung tâm.

Chuẩn hóa kernel: Sau khi tạo kernel, nó được chuẩn hóa bằng cách chia tất cả các giá trị trong kernel cho tổng của chúng. Điều này đảm bảo tổng của các giá trị trong kernel bằng 1.

Tạo một ảnh mới (output_image): Ở đây, chúng tôi tạo một ảnh mới với cùng kích thước và kiểu dữ liệu như ảnh gốc, nhưng với kiểu dữ liệu là float.

Lọc ảnh với kernel Gaussian: Duyệt qua từng điểm ảnh trong ảnh gốc (loại bỏ viền các điểm không thể áp dụng kernel) và áp dụng kernel Gaussian vào các vùng cục bộ (regions) của ảnh.

Với mỗi kênh màu (đỏ, xanh lá cây và xanh dương), chúng tôi nhân mỗi giá trị màu bởi trọng số tương ứng trong kernel và sau đó tổng hợp các giá trị này lại. Kết quả sẽ là một giá trị mới đại diện cho màu tại điểm ảnh đó.
Chuyển kiểu dữ liệu và trả về ảnh lọc nhiễu: Kết quả được chuyển về kiểu dữ liệu uint8 (số nguyên 8 bit không dấu) và trả về như kết quả của hàm gaussian_filter.

Bằng cách áp dụng kernel Gaussian này, các điểm ảnh sẽ được lọc và làm mịn, giúp giảm thiểu nhiễu và cải thiện chất lượng hình ảnh.
"""

def calculate_average_color(image):
    # image = Image.open(image_path)
    width, height = image.size

    total_red = 0
    total_green = 0
    total_blue = 0
    total_pixels = 0

    for y in range(height):
        for x in range(width):
            pixel = image.getpixel((x, y))
            total_red += pixel[0]
            total_green += pixel[1]
            total_blue += pixel[2]
            total_pixels += 1

    average_red = total_red // total_pixels
    average_green = total_green // total_pixels
    average_blue = total_blue // total_pixels

    return (average_red, average_green, average_blue)

"""Hàm này có tác dụng tính trung bình màu của tất cả các điểm ảnh trong bức ảnh cần xử lý(đánh giá độ sáng của ảnh)
hàm cũng duyệt qua tất cả các điểm ảnh và tính tổng giá trị từng loại màu và sau đó chia cho tổng số diểm ảnh để lấy được trung bình màu
"""

def process_image(image, threshold):
    width, height = image.size

    for y in range(height):
        for x in range(width):
            pixel = image.getpixel((x, y))

            if sum(pixel) > threshold:
                new_pixel = (255, 255, 255)
                image.putpixel((x, y), new_pixel)
            else:
                new_pixel = (0, 0, 0)
                image.putpixel((x, y), new_pixel)

    image = image.convert('RGB')
    return image

"""hàm này có tác dụng nhị phân hóa ảnh chuyển về ảnh đên trắng

Sau khi truyền ảnh vào hàm thì sẽ duyệt qua tất cả các điểm và nếu mà tổng 3 màu hệ RGB thang 255 > giá trị threshold thì chuyển điểm ảnh đó thành màu trắng (255, 255, 255)
"""

def trace_edges(image):
    height, width = image.shape[:2]
    edge_image = np.zeros((height, width), dtype=np.uint8)

    for y in range(1, height-1):
        for x in range(1, width-1):
            gx = int(image[y+1, x-1]) + 2*int(image[y+1, x]) + int(image[y+1, x+1]) - int(image[y-1, x-1]) - 2*int(image[y-1, x]) - int(image[y-1, x+1])
            gy = int(image[y-1, x+1]) + 2*int(image[y, x+1]) + int(image[y+1, x+1]) - int(image[y-1, x-1]) - 2*int(image[y, x-1]) - int(image[y+1, x-1])

            gradient_magnitude = np.sqrt(gx**2 + gy**2)
            edge_image[y, x] = 255 if gradient_magnitude > 230 else 0

    return edge_image

"""Hàm trace_edges sử dụng thuật toán Sobel để phát hiện các cạnh trong ảnh.
Đầu tiên, nó lặp qua mỗi pixel trong ảnh (trừ lề 1 pixel ở cạnh). Tại mỗi điểm ảnh (x, y), nó tính đạo hàm theo hướng x (gx) và theo hướng y (gy) bằng cách sử dụng kernel Sobel.

Kernel Sobel cho đạo hàm theo hướng x:

-1    0   1

-2    0   2

-1    0   1


gx = (P1 + 2*P4 + P7) - (P3 + 2*P6 + P9)

Kernel Sobel cho đạo hàm theo hướng y:

-1 -2 -1

 0  0  0

 1  2  1


 gy = (P1 + 2*P2 + P3) - (P7 + 2*P8 + P9)

Sau đó, nó tính độ lớn của gradient bằng cách sử dụng công thức Euclidean:

gradient_magnitude = sqrt(gx^2 + gy^2)

Giá trị này cho biết mức độ thay đổi của pixel tại vị trí đó. Nếu gradient_magnitude lớn, điều này ngụ ý rằng ảnh có một cạnh tại vị trí đó.
"""

# Match contours to license plate or character template
def find_contours(dimensions, img) :

    # Find all contours in the image
    cntrs, _ = cv2.findContours(img.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    # Retrieve potential dimensions
    lower_width = dimensions[0]
    upper_width = dimensions[1]
    lower_height = dimensions[2]
    upper_height = dimensions[3]

    # Check largest 5 or  15 contours for license plate or character respectively
    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]

    ii = cv2.imread('contour.jpg')

    x_cntr_list = []
    target_contours = []
    img_res = []
    for cntr in cntrs :
        # detects contour in binary image and returns the coordinates of rectangle enclosing it
        intX, intY, intWidth, intHeight = cv2.boundingRect(cntr)

        # checking the dimensions of the contour to filter out the characters by contour's size
        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :
            x_cntr_list.append(intX) #stores the x coordinate of the character's contour, to used later for indexing the contours

            char_copy = np.zeros((44,24))
            # extracting each character using the enclosing rectangle's coordinates.
            char = img[intY:intY+intHeight, intX:intX+intWidth]
            char = cv2.resize(char, (20, 40))

            cv2.rectangle(ii, (intX,intY), (intWidth+intX, intY+intHeight), (50,21,200), 2)
            plt.imshow(ii, cmap='gray')

            # Make result formatted for classification: invert colors
            char = cv2.subtract(255, char)

            # Resize the image to 24x44 with black border
            char_copy[2:42, 2:22] = char
            char_copy[0:2, :] = 0
            char_copy[:, 0:2] = 0
            char_copy[42:44, :] = 0
            char_copy[:, 22:24] = 0

            img_res.append(char_copy) # List that stores the character's binary image (unsorted)

    # Return characters on ascending order with respect to the x-coordinate (most-left character first)

    plt.show()
    # arbitrary function that stores sorted list of character indeces
    indices = sorted(range(len(x_cntr_list)), key=lambda k: x_cntr_list[k])
    img_res_copy = []
    for idx in indices:
        img_res_copy.append(img_res[idx])# stores character images according to their index
    img_res = np.array(img_res_copy)

    return img_res

# Find characters in the resulting images
def segment_characters(image) :

    # Preprocess cropped license plate image
    img_lp = cv2.resize(image, (333, 75))
    img_gray_lp = cv2.cvtColor(img_lp, cv2.COLOR_BGR2GRAY)
    _, img_binary_lp = cv2.threshold(img_gray_lp, 200, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
    img_binary_lp = cv2.erode(img_binary_lp, (3,3))
    img_binary_lp = cv2.dilate(img_binary_lp, (3,3))

    LP_WIDTH = img_binary_lp.shape[0]
    LP_HEIGHT = img_binary_lp.shape[1]

    # Make borders white
    img_binary_lp[0:3,:] = 255
    img_binary_lp[:,0:3] = 255
    img_binary_lp[72:75,:] = 255
    img_binary_lp[:,330:333] = 255

    # Estimations of character contours sizes of cropped license plates
    dimensions = [LP_WIDTH/6,
                       LP_WIDTH/2,
                       LP_HEIGHT/10,
                       2*LP_HEIGHT/3]
    plt.imshow(img_binary_lp, cmap='gray')
    plt.show()
    cv2.imwrite('contour.jpg',img_binary_lp)

    # Get contours within cropped license plate
    char_list = find_contours(dimensions, img_binary_lp)

    return char_list

image = Image.open("/content/vn7.png")
new_size = (800, 600)
resized_image = image.resize(new_size)
plt.imshow(resized_image)
plt.axis('off')  # Tắt trục tọa độ
plt.show()

smoothed_image = gaussian_filter(np.array(resized_image), kernel_size=11, sigma=9)
smoothed_image_pil = Image.fromarray(smoothed_image)
plt.imshow(smoothed_image_pil)
plt.axis('off')  # Tắt trục tọa độ
plt.show()

average_color = calculate_average_color(smoothed_image_pil)
sum_pixel = average_color[0]+average_color[1]+average_color[2]
threshold = 0.0031*sum_pixel**2 + -1.2206*sum_pixel + 480.1140
print("Threshold:", threshold)

binary_image = process_image(smoothed_image_pil, threshold)
plt.imshow(binary_image)
plt.axis('off')  # Tắt trục tọa độ
plt.show()

output_image = np.array(binary_image.convert("L"))
edges1 = trace_edges(output_image)

plt.imshow(edges1, cmap='gray')
plt.axis('off')  # Tắt trục tọa độ
plt.show()

# Assuming binary_image is a PIL Image object
binary_array = np.array(binary_image)

# Apply Canny edge detection
edged = cv2.Canny(binary_array, 20, 255)

# Display the result
cv2_imshow(edged)

cnts,new = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
cnts = sorted(cnts, key=cv2.contourArea, reverse=True)
image1=np.array(resized_image.copy())
cv2.drawContours(image1,cnts,-1,(0,255,0),1)
cv2_imshow(image1)

cnts = sorted(cnts, key = cv2.contourArea, reverse = True) [:30]
screenCnt = None
image2 = np.array(resized_image.copy())
cv2.drawContours(image2,cnts,-1,(0,255,0),1)
cv2_imshow(image2)

# Assuming image is a PIL Image object
image_array = np.array(resized_image)


for c in cnts:
    perimeter = cv2.arcLength(c, True)
    approx = cv2.approxPolyDP(c, 0.018 * perimeter, True)

    if len(approx) == 4:
        screenCnt = approx

    x, y, w, h = cv2.boundingRect(c)
    dst = image_array[y:y+h, x:x+w]
    cv2_imshow(dst)
    break

# dst = np.array(Image.open("/content/contoust.jpg"))

# Let's see the segmented characters
char = segment_characters(dst)

for i in range(10):
    if i < len(char):
        plt.subplot(1, 10, i+1)

        img = cv2.resize(char[i], (28,28), interpolation=cv2.INTER_AREA)
        # img = char[i]
        plt.imshow(img, cmap='gray')
        plt.axis('off')
        cv2.imwrite(f'/content/{i+1}.jpg' ,char[i])

# Display the original image

from tensorflow.keras.models import load_model

# Define the custom metric function
def custom_f1score(y_true, y_pred):
    # Your custom F1 score implementation here
    # Make sure to return a tensor, not a NumPy array
    return ...

# Load the model, providing the custom metric function
model = load_model('/content/drive/MyDrive/data/my_model.h5', custom_objects={'custom_f1score': custom_f1score})

# Now you should be able to use the loaded model with the custom metric function.

# Predicting the output
def fix_dimension(img):
  new_img = np.zeros((28,28,3))
  for i in range(3):
    new_img[:,:,i] = img
  return new_img

def show_results():
    dic = {}
    characters = '0123456789ABCDEFGHKLMNPSTUVXYZ'
    for i,c in enumerate(characters):
        dic[i] = c

    output = []
    for i,ch in enumerate(char): #iterating over the characters
        img_ = cv2.resize(ch, (28,28), interpolation=cv2.INTER_AREA)
        img = fix_dimension(img_)
        img = img.reshape(1,28,28,3) #preparing image for the model
        # y_ = model.predict_classes(img)[0] #predicting the class
        y_pred = model.predict(img)
        y_ = np.argmax(y_pred, axis=1)[0]

        character = dic[y_] #
        output.append(character) #storing the result in a list

    plate_number = ''.join(output)

    return plate_number

print(show_results())

# Segmented characters and their predicted value.
plt.figure(figsize=(10,6))
for i,ch in enumerate(char):
    img = cv2.resize(ch, (28,28), interpolation=cv2.INTER_AREA)
    plt.subplot(3,4,i+1)
    plt.imshow(img,cmap='gray')
    plt.title(f'predicted: {show_results()[i]}')
    plt.axis('off')
plt.show()

